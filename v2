#!/usr/bin/env python3
"""
╔══════════════════════════════════════════════════════════════════════════════╗
║  HYPERMORPHIC HOLOGRAPHIC GEARBOX - COMPLETE EDITION (UPGRADED)              ║
║  OFFLINE Dictionary+Thesaurus (WordNet+wordfreq) + SQuAD                     ║
╠══════════════════════════════════════════════════════════════════════════════╣
║  UPGRADES IN THIS EDITION                                                   ║
║  1) HOLO-RAID FIX (TRAINABLE / DIFFERENTIABLE)                              ║
║     - Replaces integer CRT residues (non-differentiable) with differentiable ║
║       prime-frequency shard encoding + shard dropout + learnable recon.      ║
║     - Keeps a separate CRT class for analysis-only (optional).               ║
║                                                                              ║
║  2) ATTENTION FIX (SOFTMAX NORMALIZATION)                                    ║
║     - Replaces sigmoid+row-norm with softmax over keys (standard, sharper).  ║
║     - Still holographic: multi-prime sin/cos score synthesis + temperature.  ║
║                                                                              ║
║  3) ROBUST QA SPAN MAPPING                                                   ║
║     - Uses sequence_ids() and offset_mapping to avoid misaligned spans.      ║
║                                                                              ║
║  4) REAL SQuAD EM/F1 EVAL (STRING-LEVEL)                                     ║
║     - Decodes predicted spans back to text and uses evaluate.load("squad").  ║
║                                                                              ║
║  5) CONTROLLED DATASET MIXING                                                ║
║     - Probabilistic batch mixing: e.g., 70% SQuAD / 30% Vocab.               ║
║                                                                              ║
║  Author: Shaun Gerrard                                                       ║
║  License: MIT                                                                ║
╚══════════════════════════════════════════════════════════════════════════════╝

GOOGLE COLAB:
1) Paste into a single cell (or split into cells where "CELL" headers are)
2) Run. It installs deps, builds offline vocab cache, trains with early stopping
"""

# ════════════════════════════════════════════════════════════════════════════════
# CELL 1: INSTALLATIONS
# ════════════════════════════════════════════════════════════════════════════════

print("█" * 80)
print("█  HYPERMORPHIC HOLOGRAPHIC GEARBOX - UPGRADED (HOLO-RAID+ATTN FIXES)" + " " * 2 + "█")
print("█" * 80)

print("\n" + "=" * 80)
print("  INSTALLING DEPENDENCIES")
print("=" * 80)

import subprocess
import sys

def install(package: str):
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

packages = [
    "torch", "torchvision", "torchaudio",
    "transformers", "datasets", "tokenizers", "sentencepiece",
    "numpy", "scipy", "matplotlib", "tqdm",
    "nltk", "wordfreq",
    "evaluate",
]

for pkg in packages:
    try:
        install(pkg)
    except Exception as e:
        print(f"  Note: {pkg} - {e}")

print("\n✓ Dependencies installed!")

# ════════════════════════════════════════════════════════════════════════════════
# CELL 2: IMPORTS AND CONFIGURATION
# ════════════════════════════════════════════════════════════════════════════════

print("\n" + "=" * 80)
print("  IMPORTING LIBRARIES")
print("=" * 80)

import os
import re
import json
import math
import time
import random
import itertools
import warnings
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Any

import numpy as np
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from datasets import load_dataset
from transformers import AutoTokenizer, get_linear_schedule_with_warmup

# Offline vocab
import nltk
from nltk.corpus import wordnet as wn
from wordfreq import top_n_list

# SQuAD metric
try:
    import evaluate
    SQUAD_METRIC = evaluate.load("squad")
except Exception as _e:
    SQUAD_METRIC = None
    print("  ⚠️ evaluate.load('squad') unavailable. Will skip EM/F1 metric.")

warnings.filterwarnings("ignore")

# Reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True

# Device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\n✓ Device: {DEVICE}")
if torch.cuda.is_available():
    print(f"  GPU: {torch.cuda.get_device_name(0)}")
    print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

PRIME_FREQUENCIES = {
    "tiny":    [3, 5, 7],
    "small":   [11, 13, 17, 19, 23],
    "medium":  [29, 31, 37, 41, 43, 47],
    "large":   [53, 59, 61, 67, 71, 73, 79],
    "default": [31, 37, 41, 43, 47],
    "holoraid":[53, 59, 61, 67, 71],
}

print("✓ All imports successful!")

# ════════════════════════════════════════════════════════════════════════════════
# CELL 3: MODEL ARCHITECTURE (HOLO-RAID FIXED + ATTENTION FIXED)
# ════════════════════════════════════════════════════════════════════════════════

print("\n" + "=" * 80)
print("  BUILDING HYPERMORPHIC HOLOGRAPHIC GEARBOX ARCHITECTURE (UPGRADED)")
print("=" * 80)

# ───────────────────────────────────────────────────────────────────────────────
# HOLO-RAID (TRAINABLE / DIFFERENTIABLE) — FIX FOR GRADIENT FLOW
# ───────────────────────────────────────────────────────────────────────────────
class HoloRAIDDifferentiable(nn.Module):
    """
    Differentiable HoloRAID:
      - Encode: prime-frequency holographic "shards" via sin/cos over x
      - Corrupt: shard dropout (simulate failures) — still differentiable
      - Decode: learnable weighted reconstruction + residual
    This behaves like fault-tolerant redundancy regularization inside the network.

    NOTE: This is the *trainable* replacement for integer CRT residues (which
    breaks gradients due to int casting and modulus).
    """
    def __init__(self,
                 dim: int,
                 primes: List[int] = None,
                 k_threshold: int = 3,
                 dropout_p: float = 0.25,
                 strength: float = 0.35):
        super().__init__()
        self.dim = dim
        self.primes = primes or PRIME_FREQUENCIES["holoraid"]
        self.n = len(self.primes)
        self.k = max(1, min(k_threshold, self.n))
        self.dropout_p = dropout_p
        self.strength = strength

        # Learnable per-shard mixing (helps reconstruction be useful)
        self.shard_mix = nn.ModuleList([nn.Linear(dim, dim, bias=False) for _ in range(self.n)])
        self.recon_mix = nn.Linear(dim, dim, bias=False)

        # Learnable per-shard weights (soft "CRT coefficients" analogue)
        self.shard_w = nn.Parameter(torch.ones(self.n))

        self.norm = nn.LayerNorm(dim)

    def _make_shard(self, x: torch.Tensor, p: int, mix: nn.Module) -> torch.Tensor:
        # x: [B,L,D]
        # Use both sin & cos for richer invertible-ish code; keep same dim by mixing.
        # Important: no floor/mod/ints.
        omega = (2.0 * math.pi) / float(p)
        s = torch.sin(omega * x)
        c = torch.cos(omega * x)
        shard = mix(s + c)
        return shard

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Build shards
        shards = []
        for i, p in enumerate(self.primes):
            shards.append(self._make_shard(x, p, self.shard_mix[i]))
        shards = torch.stack(shards, dim=0)  # [n,B,L,D]

        # Shard dropout mask (simulate failures)
        if self.training and self.dropout_p > 0:
            # keep-prob
            keep = 1.0 - self.dropout_p
            mask = (torch.rand(self.n, device=x.device) < keep).float()  # [n]
            # ensure at least k survive
            if mask.sum().item() < self.k:
                # force top-k keep (random)
                idx = torch.randperm(self.n, device=x.device)[:self.k]
                mask = torch.zeros(self.n, device=x.device)
                mask[idx] = 1.0
        else:
            mask = torch.ones(self.n, device=x.device)

        # Weighted reconstruction
        w = F.softmax(self.shard_w, dim=0) * mask
        denom = w.sum().clamp_min(1e-6)
        w = w / denom

        recon = (w.view(self.n, 1, 1, 1) * shards).sum(dim=0)  # [B,L,D]
        recon = self.recon_mix(recon)

        # Residual + norm
        out = self.norm(x + self.strength * recon)
        return out


# ───────────────────────────────────────────────────────────────────────────────
# OPTIONAL: CRT-STYLE HOLO-RAID (ANALYSIS ONLY) — NON-DIFFERENTIABLE
# ───────────────────────────────────────────────────────────────────────────────
class HoloRAIDCRT(nn.Module):
    """
    Non-differentiable CRT residues (analysis/testing only).
    Keep this out of training graphs. Use with torch.no_grad().
    """
    def __init__(self, n_shards=5, k_threshold=3, primes=None, scale=100.0):
        super().__init__()
        self.n = n_shards
        self.k = k_threshold
        self.scale = scale
        self.primes = (primes or PRIME_FREQUENCIES["holoraid"])[:n_shards]
        self.register_buffer("prime_tensor", torch.tensor(self.primes, dtype=torch.float32))

    def _mod_inverse(self, a: int, m: int) -> int:
        def egcd(a, b):
            if b == 0:
                return a, 1, 0
            g, x, y = egcd(b, a % b)
            return g, y, x - (a // b) * y
        g, x, _ = egcd(a % m, m)
        if g != 1:
            raise ValueError("No inverse")
        return x % m

    def _coeffs(self, indices):
        ps = [self.primes[i] for i in indices]
        M = 1
        for p in ps:
            M *= p
        Mi = [M // p for p in ps]
        yi = [self._mod_inverse(Mi[j], ps[j]) for j in range(len(ps))]
        return M, Mi, yi

    def encode(self, x: torch.Tensor) -> List[torch.Tensor]:
        x_shift = x - x.min() + 1
        x_int = (x_shift * self.scale).long()
        shards = [(x_int % p).float() / p for p in self.primes]
        self._last_min = x.min().item()
        return shards

    def decode(self, shards: List[torch.Tensor], indices=None) -> torch.Tensor:
        if indices is None:
            indices = list(range(self.k))
        indices = sorted(indices[:self.k])
        M, Mi, yi = self._coeffs(indices)
        out = torch.zeros_like(shards[0])
        for j, idx in enumerate(indices):
            p = self.primes[idx]
            r = (shards[idx] * p).round()
            out = out + r * Mi[j] * yi[j]
        out = torch.fmod(out, float(M)) / self.scale
        out = out + self._last_min - 1
        return out


# ───────────────────────────────────────────────────────────────────────────────
# SAFEGEAR
# ───────────────────────────────────────────────────────────────────────────────
class SafeGear(nn.Module):
    def __init__(self, dim: int, n_gears: int = 3):
        super().__init__()
        self.dim = dim
        self.n_gears = n_gears
        self.gear_a = nn.Parameter(torch.ones(n_gears) * 0.5)
        self.gear_b = nn.Parameter(torch.ones(n_gears) * 7.0)
        self.mix = nn.Linear(dim, dim)
        self.norm = nn.LayerNorm(dim)

    def gear_transform(self, x: torch.Tensor, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
        a_pos = F.softplus(a) + 0.1
        b_pos = F.softplus(b) + 2.0
        mod_like = torch.sin(2 * math.pi * x / b_pos) * a_pos
        div_like = x / b_pos
        return mod_like + div_like

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = x
        for i in range(self.n_gears):
            out = self.gear_transform(out, self.gear_a[i], self.gear_b[i])
        out = self.mix(out)
        return self.norm(out + x)


# ───────────────────────────────────────────────────────────────────────────────
# HOLOMIX
# ───────────────────────────────────────────────────────────────────────────────
class HoloMix(nn.Module):
    def __init__(self, dim: int, hidden_dim: int = None,
                 frequencies: List[int] = None, alpha: float = 0.30, dropout: float = 0.1):
        super().__init__()
        self.dim = dim
        self.hidden_dim = hidden_dim or dim * 4
        self.frequencies = frequencies or PRIME_FREQUENCIES["default"]
        self.n_freq = len(self.frequencies)
        self.alpha = alpha

        self.W1 = nn.Linear(dim, self.hidden_dim)
        self.W2 = nn.Linear(self.hidden_dim, dim)

        self.amp = nn.Parameter(torch.randn(self.n_freq, self.hidden_dim) * 0.08)
        self.phi = nn.Parameter(torch.rand(self.n_freq, self.hidden_dim) * 2 * math.pi)

        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        h = self.W1(x)
        inter = torch.zeros_like(h)
        for i, p in enumerate(self.frequencies):
            inter = inter + self.amp[i] * torch.sin((2 * math.pi * h / p) + self.phi[i])
        h = h + self.alpha * inter
        h = F.gelu(h)
        h = self.dropout(h)
        out = self.W2(h)
        return self.norm(out)


# ───────────────────────────────────────────────────────────────────────────────
# HOLOGRAPHIC ATTENTION (FIXED: SOFTMAX)
# ───────────────────────────────────────────────────────────────────────────────
class HolographicAttention(nn.Module):
    """
    Fix: use softmax(scores) over keys, instead of sigmoid + row-normalize.
    Upgrade: sin+cos holographic synthesis per prime frequency.
    """
    def __init__(self, dim: int, n_heads: int = 4, dropout: float = 0.1, frequencies: List[int] = None):
        super().__init__()
        assert dim % n_heads == 0, "dim must be divisible by n_heads"
        self.dim = dim
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        self.frequencies = frequencies or PRIME_FREQUENCIES["default"]
        self.n_freq = len(self.frequencies)

        self.Wq = nn.Linear(dim, dim)
        self.Wk = nn.Linear(dim, dim)
        self.Wv = nn.Linear(dim, dim)
        self.Wo = nn.Linear(dim, dim)

        # Separate sin/cos amplitudes for richer score synthesis
        self.alpha_sin = nn.Parameter(torch.randn(n_heads, self.n_freq) * 0.10)
        self.alpha_cos = nn.Parameter(torch.randn(n_heads, self.n_freq) * 0.10)
        self.phi = nn.Parameter(torch.rand(n_heads, self.n_freq) * 2 * math.pi)

        self.temperature = nn.Parameter(torch.ones(1))

        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(dim)

        self.last_attention_weights = None

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        B, L, D = x.shape

        Q = self.Wq(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)  # [B,H,L,hd]
        K = self.Wk(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)
        V = self.Wv(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)

        sim = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B,H,L,L]

        temp = self.temperature.abs() + 0.1
        sim = sim / temp

        scores = torch.zeros_like(sim)
        for f_idx, p in enumerate(self.frequencies):
            phase = (2 * math.pi * sim / p) + self.phi[:, f_idx].view(1, self.n_heads, 1, 1)
            scores = scores + self.alpha_sin[:, f_idx].view(1, self.n_heads, 1, 1) * torch.sin(phase)
            scores = scores + self.alpha_cos[:, f_idx].view(1, self.n_heads, 1, 1) * torch.cos(phase)

        if mask is not None:
            # mask expected shape [B,1,1,L] or [B,L]
            if mask.dim() == 2:
                mask = mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(mask == 0, -1e9)

        # FIX: softmax
        weights = torch.softmax(scores, dim=-1)
        self.last_attention_weights = weights.detach()

        weights = self.dropout(weights)
        out = torch.matmul(weights, V)  # [B,H,L,hd]
        out = out.transpose(1, 2).contiguous().view(B, L, D)
        out = self.Wo(out)
        return self.norm(out)


class HolographicTransformerBlock(nn.Module):
    def __init__(self, dim: int, n_heads: int = 4, dropout: float = 0.1, frequencies: List[int] = None, ffn_mult: int = 4):
        super().__init__()
        self.attn = HolographicAttention(dim, n_heads, dropout, frequencies)
        self.ffn = HoloMix(dim, dim * ffn_mult, frequencies, dropout=dropout)
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        self.drop = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        h = self.attn(self.norm1(x), mask)
        x = x + self.drop(h)
        h = self.ffn(self.norm2(x))
        x = x + self.drop(h)
        return x


class HolographicGearboxModel(nn.Module):
    def __init__(self,
                 vocab_size: int,
                 dim: int = 256,
                 n_layers: int = 4,
                 n_heads: int = 4,
                 max_seq_len: int = 384,
                 dropout: float = 0.2,
                 frequencies: List[int] = None,
                 # HOLO-RAID (differentiable) controls
                 use_holoraid: bool = True,
                 holoraid_prob: float = 0.10,
                 holoraid_dropout_p: float = 0.25,
                 holoraid_strength: float = 0.35):
        super().__init__()
        self.dim = dim
        self.max_seq_len = max_seq_len
        self.frequencies = frequencies or PRIME_FREQUENCIES["default"]
        self.use_holoraid = use_holoraid
        self.holoraid_prob = holoraid_prob

        self.token_embed = nn.Embedding(vocab_size, dim)
        self.safegear = SafeGear(dim, n_gears=3)

        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, dim))
        self._init_positions()

        # FIXED: differentiable HoloRAID (trainable)
        self.holoraid = HoloRAIDDifferentiable(
            dim=dim,
            primes=PRIME_FREQUENCIES["holoraid"],
            k_threshold=3,
            dropout_p=holoraid_dropout_p,
            strength=holoraid_strength,
        )

        self.layers = nn.ModuleList([
            HolographicTransformerBlock(dim, n_heads, dropout, self.frequencies)
            for _ in range(n_layers)
        ])
        self.norm = nn.LayerNorm(dim)
        self.drop = nn.Dropout(dropout)

        self.qa_head = nn.Linear(dim, 2)

        self._init_weights()

    def _init_positions(self):
        pos = torch.arange(self.max_seq_len).float()
        pe = torch.zeros(self.max_seq_len, self.dim)

        # prime holographic
        for i, p in enumerate(self.frequencies):
            if 2*i < self.dim:
                pe[:, 2*i] = torch.sin(2 * math.pi * pos / p)
            if 2*i+1 < self.dim:
                pe[:, 2*i+1] = torch.cos(2 * math.pi * pos / p)

        # remaining dims: standard sinusoid
        start = 2 * len(self.frequencies)
        for i in range(start, self.dim, 2):
            div = 10000 ** (i / self.dim)
            pe[:, i] = torch.sin(pos / div)
            if i+1 < self.dim:
                pe[:, i+1] = torch.cos(pos / div)

        self.pos_embed.data = pe.unsqueeze(0)

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Embedding):
                nn.init.normal_(m.weight, std=0.02)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        B, L = input_ids.shape
        x = self.token_embed(input_ids)
        x = self.safegear(x)
        x = x + self.pos_embed[:, :L, :]

        # FIXED: apply differentiable HoloRAID only probabilistically
        if self.use_holoraid and self.training and (random.random() < self.holoraid_prob):
            x = self.holoraid(x)

        x = self.drop(x)

        mask = attention_mask.unsqueeze(1).unsqueeze(2) if attention_mask is not None else None
        for layer in self.layers:
            x = layer(x, mask)

        x = self.norm(x)
        logits = self.qa_head(x)  # [B,L,2]
        return {
            "start_logits": logits[:, :, 0],
            "end_logits": logits[:, :, 1],
            "hidden_states": x
        }

    def get_num_parameters(self) -> int:
        return sum(p.numel() for p in self.parameters())

    def get_attention_weights(self, layer_idx: int = -1) -> Optional[torch.Tensor]:
        if layer_idx == -1:
            layer_idx = len(self.layers) - 1
        if 0 <= layer_idx < len(self.layers):
            return self.layers[layer_idx].attn.last_attention_weights
        return None


print("✓ Model architecture defined (Upgraded HoloRAID + Softmax Attention).")

# ════════════════════════════════════════════════════════════════════════════════
# CELL 4: DATA (OFFLINE VOCAB + SQuAD) WITH ROBUST SPAN MAPPING
# ════════════════════════════════════════════════════════════════════════════════

print("\n" + "=" * 80)
print("  LOADING DATA SOURCES (OFFLINE VOCAB + SQuAD)")
print("=" * 80)

def ensure_wordnet():
    try:
        wn.synsets("test")
    except LookupError:
        nltk.download("wordnet")
        nltk.download("omw-1.4")

_word_re = re.compile(r"^[a-z][a-z\-']*$")

def wordnet_entry(word: str, max_syn=12, max_ant=8, max_rel=12) -> Optional[Dict[str, Any]]:
    synsets = wn.synsets(word)
    if not synsets:
        return None

    defs: List[str] = []
    for s in synsets[:3]:
        d = s.definition()
        if d and d not in defs:
            defs.append(d)

    synonyms: List[str] = []
    antonyms: List[str] = []
    related: List[str] = []

    for s in synsets:
        for lemma in s.lemmas():
            w = lemma.name().replace("_", " ").lower()
            if w != word and w not in synonyms:
                synonyms.append(w)
            for ant in lemma.antonyms():
                a = ant.name().replace("_", " ").lower()
                if a not in antonyms:
                    antonyms.append(a)

        # small sample hypernyms/hyponyms
        for h in s.hypernyms()[:3]:
            for lemma in h.lemmas()[:3]:
                rw = lemma.name().replace("_", " ").lower()
                if rw != word and rw not in related:
                    related.append(rw)
        for h in s.hyponyms()[:3]:
            for lemma in h.lemmas()[:3]:
                rw = lemma.name().replace("_", " ").lower()
                if rw != word and rw not in related:
                    related.append(rw)

    entry: Dict[str, Any] = {"word": word}
    if defs: entry["definitions"] = defs
    if synonyms: entry["synonyms"] = synonyms[:max_syn]
    if antonyms: entry["antonyms"] = antonyms[:max_ant]
    if related: entry["related"] = related[:max_rel]
    return entry if len(entry) > 1 else None

def load_vocab_cache_jsonl(path: str, limit: int) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        return []
    out = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                out.append(json.loads(line))
            except:
                continue
            if len(out) >= limit:
                break
    return out

def save_vocab_cache_jsonl(path: str, entries: List[Dict[str, Any]]):
    with open(path, "w", encoding="utf-8") as f:
        for e in entries:
            f.write(json.dumps(e, ensure_ascii=False) + "\n")

def build_massive_vocab_dataset(
    target_size: int = 50000,
    min_word_len: int = 3,
    top_k_wordfreq: int = 200000,
    cache_path: str = "wordnet_vocab_cache.jsonl",
) -> List[Dict[str, Any]]:
    ensure_wordnet()

    cached = load_vocab_cache_jsonl(cache_path, target_size)
    if len(cached) >= target_size:
        print(f"  ✓ Loaded vocab cache: {len(cached)} entries from {cache_path}")
        return cached[:target_size]

    if cached:
        print(f"  ✓ Partial cache found: {len(cached)} entries. Continuing build...")

    candidates = top_n_list("en", top_k_wordfreq)

    dataset = list(cached)
    seen = {d["word"] for d in dataset if "word" in d}

    for w in tqdm(candidates, desc="Building WordNet vocab"):
        w = w.strip().lower()
        if w in seen:
            continue
        if len(w) < min_word_len:
            continue
        if not _word_re.match(w):
            continue

        seen.add(w)
        entry = wordnet_entry(w)
        if entry:
            dataset.append(entry)

        if len(dataset) >= target_size:
            break

    save_vocab_cache_jsonl(cache_path, dataset)
    print(f"  ✓ Saved vocab cache: {len(dataset)} entries → {cache_path}")
    return dataset

def find_answer_token_span(enc, answer_start: int, answer_text: str) -> Tuple[int, int]:
    """
    Robust span mapping:
      - Only consider context tokens (sequence_id == 1)
      - Returns CLS (0) if not found (e.g., truncation)
    """
    answer_end = answer_start + len(answer_text)
    offsets = enc["offset_mapping"][0].tolist()
    seq_ids = enc.sequence_ids(0)

    cls_index = 0
    start_pos = cls_index
    end_pos = cls_index

    for i, (s, e) in enumerate(offsets):
        if seq_ids[i] != 1:
            continue
        if s == e == 0:
            continue
        if s <= answer_start < e:
            start_pos = i
        if s < answer_end <= e:
            end_pos = i
            break

    if end_pos < start_pos:
        end_pos = start_pos
    return start_pos, end_pos

class VocabularyQADataset(Dataset):
    def __init__(self, vocab_data: List[Dict[str, Any]], tokenizer, max_length: int = 384,
                 max_syn_show: int = 5, max_ant_show: int = 3, max_rel_show: int = 5):
        self.features: List[Dict[str, Any]] = []
        self.tokenizer = tokenizer
        self.max_length = max_length

        samples = []
        for entry in vocab_data:
            word = entry.get("word", "")
            if not word:
                continue

            defs = entry.get("definitions", [])
            if defs:
                definition = defs[0]
                ctx = f"The word {word} is defined as: {definition}"
                q = f"What is the definition of {word}?"
                samples.append((q, ctx, definition, len(f"The word {word} is defined as: ")))

            syns = entry.get("synonyms", [])
            if syns:
                ans = ", ".join(syns[:max_syn_show])
                ctx = f"Synonyms of {word} include: {ans}. These words have similar meanings."
                q = f"What are synonyms of {word}?"
                samples.append((q, ctx, ans, len(f"Synonyms of {word} include: ")))

            ants = entry.get("antonyms", [])
            if ants:
                ans = ", ".join(ants[:max_ant_show])
                ctx = f"The opposite of {word} is: {ans}. These are antonyms."
                q = f"What is the opposite of {word}?"
                samples.append((q, ctx, ans, len(f"The opposite of {word} is: ")))

            rel = entry.get("related", [])
            if rel:
                ans = ", ".join(rel[:max_rel_show])
                ctx = f"Words related to {word} include: {ans}. They share semantic connections."
                q = f"What words are related to {word}?"
                samples.append((q, ctx, ans, len(f"Words related to {word} include: ")))

        for q, ctx, ans, ans_start in tqdm(samples, desc="Tokenizing Vocab QA"):
            enc = tokenizer(
                q, ctx,
                max_length=max_length,
                truncation="only_second",
                padding="max_length",
                return_offsets_mapping=True,
                return_tensors="pt",
            )
            sp, ep = find_answer_token_span(enc, ans_start, ans)
            self.features.append({
                "input_ids": enc["input_ids"].squeeze(0),
                "attention_mask": enc["attention_mask"].squeeze(0),
                "start_positions": sp,
                "end_positions": ep,
            })

    def __len__(self):
        return len(self.features)

    def __getitem__(self, i):
        f = self.features[i]
        return {
            "input_ids": f["input_ids"],
            "attention_mask": f["attention_mask"],
            "start_positions": f["start_positions"],
            "end_positions": f["end_positions"],
        }

class SQuADTrainDataset(Dataset):
    def __init__(self, tokenizer, split="train", max_length=384, max_samples=None):
        dataset = load_dataset("squad", split=split)
        if max_samples:
            dataset = dataset.select(range(min(max_samples, len(dataset))))

        self.features: List[Dict[str, Any]] = []
        self.tokenizer = tokenizer
        self.max_length = max_length

        for ex in tqdm(dataset, desc=f"Tokenizing SQuAD ({split})"):
            enc = tokenizer(
                ex["question"], ex["context"],
                max_length=max_length,
                truncation="only_second",
                padding="max_length",
                return_offsets_mapping=True,
                return_tensors="pt",
            )

            cls = 0
            sp, ep = cls, cls
            if ex["answers"]["answer_start"]:
                astart = ex["answers"]["answer_start"][0]
                atext = ex["answers"]["text"][0]
                sp, ep = find_answer_token_span(enc, astart, atext)

            self.features.append({
                "input_ids": enc["input_ids"].squeeze(0),
                "attention_mask": enc["attention_mask"].squeeze(0),
                "start_positions": sp,
                "end_positions": ep,
            })

    def __len__(self):
        return len(self.features)

    def __getitem__(self, i):
        f = self.features[i]
        return {
            "input_ids": f["input_ids"],
            "attention_mask": f["attention_mask"],
            "start_positions": f["start_positions"],
            "end_positions": f["end_positions"],
        }

class SQuADValDataset(Dataset):
    """
    Validation dataset retains context + offset_mapping for string-level EM/F1.
    """
    def __init__(self, tokenizer, split="validation", max_length=384, max_samples=None):
        dataset = load_dataset("squad", split=split)
        if max_samples:
            dataset = dataset.select(range(min(max_samples, len(dataset))))

        self.items: List[Dict[str, Any]] = []
        self.tokenizer = tokenizer
        self.max_length = max_length

        for ex in tqdm(dataset, desc=f"Tokenizing SQuAD ({split})"):
            enc = tokenizer(
                ex["question"], ex["context"],
                max_length=max_length,
                truncation="only_second",
                padding="max_length",
                return_offsets_mapping=True,
                return_tensors="pt",
            )

            cls = 0
            sp, ep = cls, cls
            if ex["answers"]["answer_start"]:
                astart = ex["answers"]["answer_start"][0]
                atext = ex["answers"]["text"][0]
                sp, ep = find_answer_token_span(enc, astart, atext)

            self.items.append({
                "input_ids": enc["input_ids"].squeeze(0),
                "attention_mask": enc["attention_mask"].squeeze(0),
                "start_positions": sp,
                "end_positions": ep,
                "offset_mapping": enc["offset_mapping"].squeeze(0),  # [L,2]
                "context": ex["context"],
                "id": ex["id"],
                "answers": ex["answers"],  # {"text":[...], "answer_start":[...]}
                # We also need seq_ids for decoding; recompute from tokenizer fast path:
                "question": ex["question"],
            })

    def __len__(self):
        return len(self.items)

    def __getitem__(self, i):
        x = self.items[i]
        return {
            "input_ids": x["input_ids"],
            "attention_mask": x["attention_mask"],
            "start_positions": x["start_positions"],
            "end_positions": x["end_positions"],
            "offset_mapping": x["offset_mapping"],
            "context": x["context"],
            "id": x["id"],
            "answers": x["answers"],
            "question": x["question"],
        }

print("\n  Initializing tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", use_fast=True)

VOCAB_CACHE_PATH = "wordnet_vocab_cache.jsonl"
VOCAB_TARGET_ENTRIES = 50000
VOCAB_WORD_FREQ_CANDIDATES = 200000

print("\n  Building offline dictionary + thesaurus (WordNet)…")
vocab_data = build_massive_vocab_dataset(
    target_size=VOCAB_TARGET_ENTRIES,
    top_k_wordfreq=VOCAB_WORD_FREQ_CANDIDATES,
    cache_path=VOCAB_CACHE_PATH
)
print(f"  ✓ Loaded {len(vocab_data)} WordNet vocab entries")

vocab_dataset = VocabularyQADataset(vocab_data, tokenizer, max_length=384)
print(f"  ✓ Created {len(vocab_dataset)} vocab QA samples")

MAX_TRAIN_SAMPLES = 10000
MAX_VAL_SAMPLES   = 2000

squad_train = SQuADTrainDataset(tokenizer, "train", max_length=384, max_samples=MAX_TRAIN_SAMPLES)
squad_val   = SQuADValDataset(tokenizer, "validation", max_length=384, max_samples=MAX_VAL_SAMPLES)

print(f"  ✓ Loaded SQuAD: {len(squad_train)} train, {len(squad_val)} validation")

# ════════════════════════════════════════════════════════════════════════════════
# CELL 5: TRAINING (CONTROLLED MIXING + EARLY STOPPING + REAL EM/F1)
# ════════════════════════════════════════════════════════════════════════════════

print("\n" + "=" * 80)
print("  TRAINING CONFIGURATION")
print("=" * 80)

@dataclass
class TrainingConfig:
    # Model
    vocab_size: int = 30522
    dim: int = 256
    n_layers: int = 4
    n_heads: int = 4
    max_seq_len: int = 384
    dropout: float = 0.2

    # HoloRAID (differentiable)
    use_holoraid: bool = True
    holoraid_prob: float = 0.10
    holoraid_dropout_p: float = 0.25
    holoraid_strength: float = 0.35

    # Train
    batch_size: int = 16
    epochs: int = 50
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    warmup_ratio: float = 0.10
    max_grad_norm: float = 1.0

    # Mixing
    vocab_batch_ratio: float = 0.30     # 30% vocab, 70% SQuAD
    steps_per_epoch: int = 800

    # Eval speed control
    eval_max_examples: int = 600        # EM/F1 computed on subset for speed

    # Early stopping
    patience: int = 5
    min_delta: float = 0.001

    save_best: bool = True
    log_every: int = 50

config = TrainingConfig()

if not torch.cuda.is_available():
    config.batch_size = 4
    config.dim = 128
    config.n_layers = 2
    config.epochs = 10
    config.steps_per_epoch = 200
    config.eval_max_examples = 200
    print("  ⚠️ CPU mode - reduced configuration")

print(f"\n  Mix: {int((1-config.vocab_batch_ratio)*100)}% SQuAD / {int(config.vocab_batch_ratio*100)}% Vocab")
print(f"  Steps/epoch cap: {config.steps_per_epoch}")

print("\n  Initializing model...")
model = HolographicGearboxModel(
    vocab_size=config.vocab_size,
    dim=config.dim,
    n_layers=config.n_layers,
    n_heads=config.n_heads,
    max_seq_len=config.max_seq_len,
    dropout=config.dropout,
    frequencies=PRIME_FREQUENCIES["default"],
    use_holoraid=config.use_holoraid,
    holoraid_prob=config.holoraid_prob,
    holoraid_dropout_p=config.holoraid_dropout_p,
    holoraid_strength=config.holoraid_strength,
).to(DEVICE)

print(f"  ✓ Model params: {model.get_num_parameters():,}")

def collate_train(batch):
    return {
        "input_ids": torch.stack([b["input_ids"] for b in batch]),
        "attention_mask": torch.stack([b["attention_mask"] for b in batch]),
        "start_positions": torch.tensor([b["start_positions"] for b in batch], dtype=torch.long),
        "end_positions": torch.tensor([b["end_positions"] for b in batch], dtype=torch.long),
    }

def collate_val(batch):
    # keep python objects for decoding
    return {
        "input_ids": torch.stack([b["input_ids"] for b in batch]),
        "attention_mask": torch.stack([b["attention_mask"] for b in batch]),
        "start_positions": torch.tensor([b["start_positions"] for b in batch], dtype=torch.long),
        "end_positions": torch.tensor([b["end_positions"] for b in batch], dtype=torch.long),
        "offset_mapping": [b["offset_mapping"] for b in batch],
        "context": [b["context"] for b in batch],
        "id": [b["id"] for b in batch],
        "answers": [b["answers"] for b in batch],
        "question": [b["question"] for b in batch],
    }

squad_loader = DataLoader(squad_train, batch_size=config.batch_size, shuffle=True, collate_fn=collate_train, num_workers=0)
vocab_loader = DataLoader(vocab_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_train, num_workers=0)
val_loader   = DataLoader(squad_val,   batch_size=config.batch_size, shuffle=False, collate_fn=collate_val,   num_workers=0)

# Optimizer
no_decay = ["bias", "LayerNorm.weight", "norm.weight"]
params_decay = []
params_nodecay = []
for n, p in model.named_parameters():
    if any(nd in n for nd in no_decay):
        params_nodecay.append(p)
    else:
        params_decay.append(p)

optimizer = torch.optim.AdamW(
    [{"params": params_decay, "weight_decay": config.weight_decay},
     {"params": params_nodecay, "weight_decay": 0.0}],
    lr=config.learning_rate
)

total_steps = config.steps_per_epoch * config.epochs
warmup_steps = int(total_steps * config.warmup_ratio)
scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)

LR_HISTORY = []

def mixed_batch_stream(squad_loader, vocab_loader, vocab_ratio: float):
    squad_iter = itertools.cycle(squad_loader)
    vocab_iter = itertools.cycle(vocab_loader)
    while True:
        if random.random() < vocab_ratio:
            yield "vocab", next(vocab_iter)
        else:
            yield "squad", next(squad_iter)

def train_epoch(epoch: int) -> float:
    model.train()
    total_loss = 0.0
    n = 0
    stream = mixed_batch_stream(squad_loader, vocab_loader, config.vocab_batch_ratio)
    pbar = tqdm(range(config.steps_per_epoch), desc=f"Epoch {epoch+1}/{config.epochs}")

    for step in pbar:
        src, batch = next(stream)
        input_ids = batch["input_ids"].to(DEVICE)
        attn = batch["attention_mask"].to(DEVICE)
        sp = batch["start_positions"].to(DEVICE)
        ep = batch["end_positions"].to(DEVICE)

        out = model(input_ids, attn)
        loss = 0.5 * (F.cross_entropy(out["start_logits"], sp) + F.cross_entropy(out["end_logits"], ep))

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

        lr = scheduler.get_last_lr()[0]
        LR_HISTORY.append(lr)

        total_loss += loss.item()
        n += 1

        if (step + 1) % config.log_every == 0:
            pbar.set_postfix({"src": src, "loss": f"{loss.item():.4f}", "avg": f"{total_loss/n:.4f}", "lr": f"{lr:.2e}"})
        else:
            pbar.set_postfix({"src": src, "avg": f"{total_loss/n:.4f}", "lr": f"{lr:.2e}"})

    return total_loss / max(n, 1)

def best_span_from_logits(start_logits: np.ndarray,
                          end_logits: np.ndarray,
                          ctx_token_indices: List[int],
                          max_answer_len: int = 30,
                          n_best: int = 20) -> Tuple[int, int]:
    """
    Choose best (start,end) restricted to context tokens using logits.
    """
    # take top candidates among context tokens
    s_idx_sorted = sorted(ctx_token_indices, key=lambda i: start_logits[i], reverse=True)[:n_best]
    e_idx_sorted = sorted(ctx_token_indices, key=lambda i: end_logits[i], reverse=True)[:n_best]

    best_score = -1e18
    best_s, best_e = ctx_token_indices[0], ctx_token_indices[0]

    for s in s_idx_sorted:
        for e in e_idx_sorted:
            if e < s:
                continue
            if (e - s + 1) > max_answer_len:
                continue
            score = start_logits[s] + end_logits[e]
            if score > best_score:
                best_score = score
                best_s, best_e = s, e

    return best_s, best_e

def decode_batch_predictions(batch, start_logits_t, end_logits_t) -> Tuple[List[Dict[str, str]], List[Dict[str, Any]]]:
    """
    Decode predicted spans to text (for squad EM/F1).
    Returns:
      predictions: [{"id": ..., "prediction_text": ...}, ...]
      references:  [{"id": ..., "answers": {...}}, ...]
    """
    preds = []
    refs = []

    # We need sequence_ids for each item; compute using tokenizer on the fly (val set is small).
    # This ensures correctness even if tokenizer behavior differs.
    for i in range(len(batch["id"])):
        q = batch["question"][i]
        ctx = batch["context"][i]
        off = batch["offset_mapping"][i].cpu().numpy().tolist()  # [L,2]
        # recompute seq_ids
        enc = tokenizer(
            q, ctx,
            max_length=config.max_seq_len,
            truncation="only_second",
            padding="max_length",
            return_offsets_mapping=False,
            return_tensors="pt"
        )
        seq_ids = tokenizer(q, ctx, max_length=config.max_seq_len, truncation="only_second",
                            padding="max_length").sequence_ids()

        start_logits = start_logits_t[i].detach().cpu().numpy()
        end_logits   = end_logits_t[i].detach().cpu().numpy()

        # context token indices
        ctx_tok = [t for t, sid in enumerate(seq_ids) if sid == 1]

        if len(ctx_tok) == 0:
            pred_text = ""
        else:
            s, e = best_span_from_logits(start_logits, end_logits, ctx_tok)
            s_char, _ = off[s]
            _, e_char = off[e]
            if s_char == 0 and e_char == 0:
                pred_text = ""
            else:
                pred_text = ctx[s_char:e_char].strip()

        preds.append({"id": batch["id"][i], "prediction_text": pred_text})
        refs.append({"id": batch["id"][i], "answers": batch["answers"][i]})

    return preds, refs

def evaluate_model() -> Dict[str, float]:
    model.eval()
    total_loss = 0.0
    total = 0
    tok_em = 0
    start_acc = 0
    end_acc = 0

    all_preds = []
    all_refs = []

    with torch.no_grad():
        seen = 0
        for batch in tqdm(val_loader, desc="Evaluating", leave=False):
            input_ids = batch["input_ids"].to(DEVICE)
            attn      = batch["attention_mask"].to(DEVICE)
            sp        = batch["start_positions"].to(DEVICE)
            ep        = batch["end_positions"].to(DEVICE)

            out = model(input_ids, attn)
            loss = 0.5 * (F.cross_entropy(out["start_logits"], sp) + F.cross_entropy(out["end_logits"], ep))
            bs = input_ids.size(0)

            total_loss += loss.item() * bs
            total += bs

            s_pred = out["start_logits"].argmax(dim=-1)
            e_pred = out["end_logits"].argmax(dim=-1)

            start_acc += (s_pred == sp).sum().item()
            end_acc   += (e_pred == ep).sum().item()
            tok_em    += ((s_pred == sp) & (e_pred == ep)).sum().item()

            # Real EM/F1 decode subset
            if SQUAD_METRIC is not None and seen < config.eval_max_examples:
                preds, refs = decode_batch_predictions(batch, out["start_logits"], out["end_logits"])
                all_preds.extend(preds)
                all_refs.extend(refs)
                seen += bs

    metrics = {
        "loss": total_loss / max(total, 1),
        "start_acc_tok": start_acc / max(total, 1),
        "end_acc_tok": end_acc / max(total, 1),
        "exact_match_tok": tok_em / max(total, 1),
    }

    if SQUAD_METRIC is not None and all_preds:
        squad_scores = SQUAD_METRIC.compute(predictions=all_preds, references=all_refs)
        metrics["squad_em"] = float(squad_scores["exact_match"])
        metrics["squad_f1"] = float(squad_scores["f1"])

    return metrics

print("\n" + "=" * 80)
print("  STARTING TRAINING (EARLY STOPPING)")
print("=" * 80)

train_losses = []
val_hist = []
best_val = float("inf")
best_em = -1.0
best_epoch = 0
pat = 0

for epoch in range(config.epochs):
    tr = train_epoch(epoch)
    train_losses.append(tr)

    vm = evaluate_model()
    val_hist.append(vm)

    print(f"\n  Epoch {epoch+1}/{config.epochs} Summary:")
    print(f"    Train Loss:       {tr:.4f}")
    print(f"    Val Loss:         {vm['loss']:.4f}")
    print(f"    Token Start Acc:  {vm['start_acc_tok']:.4f}")
    print(f"    Token End Acc:    {vm['end_acc_tok']:.4f}")
    print(f"    Token EM (idx):   {vm['exact_match_tok']:.4f}")
    if "squad_em" in vm:
        print(f"    SQuAD EM (text):  {vm['squad_em']:.2f}")
        print(f"    SQuAD F1 (text):  {vm['squad_f1']:.2f}")

    improved = False
    if vm["loss"] < best_val - config.min_delta:
        best_val = vm["loss"]
        improved = True

    # prefer real squad_em if available
    cur_em = vm.get("squad_em", vm["exact_match_tok"])
    if cur_em > best_em:
        best_em = cur_em
        improved = True

    if improved:
        best_epoch = epoch + 1
        pat = 0
        if config.save_best:
            torch.save({
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "val_metrics": vm,
                "config": config.__dict__,
                "vocab_cache_path": VOCAB_CACHE_PATH,
                "vocab_entries": len(vocab_data),
                "vocab_samples": len(vocab_dataset),
            }, "holographic_gearbox_best.pt")
            print(f"    ✓ New best model saved! (val_loss={best_val:.4f}, best_em={best_em:.4f})")
    else:
        pat += 1
        print(f"    No improvement ({pat}/{config.patience})")

    if pat >= config.patience:
        print(f"\n  Early stopping at epoch {epoch+1}. Best epoch: {best_epoch}")
        break

print("\n✓ Training complete!")

# ════════════════════════════════════════════════════════════════════════════════
# CELL 6: VISUALIZATION + QUICK ATTENTION VIEW
# ════════════════════════════════════════════════════════════════════════════════

print("\n" + "=" * 80)
print("  VISUALIZATION AND ANALYSIS")
print("=" * 80)

epochs_range = range(1, len(train_losses) + 1)

val_loss_curve = [m["loss"] for m in val_hist]
tok_em_curve = [m["exact_match_tok"] for m in val_hist]
squad_em_curve = [m.get("squad_em", None) for m in val_hist]
squad_f1_curve = [m.get("squad_f1", None) for m in val_hist]

fig, axes = plt.subplots(2, 2, figsize=(14, 9))

# Loss
axes[0,0].plot(list(epochs_range), train_losses, linewidth=2, label="Train Loss")
axes[0,0].plot(list(epochs_range), val_loss_curve, linewidth=2, label="Val Loss")
axes[0,0].axvline(best_epoch, linestyle="--", label=f"Best ({best_epoch})")
axes[0,0].set_title("Loss")
axes[0,0].set_xlabel("Epoch")
axes[0,0].set_ylabel("Loss")
axes[0,0].grid(True, alpha=0.3)
axes[0,0].legend()

# Token Acc
axes[0,1].plot(list(epochs_range), [m["start_acc_tok"] for m in val_hist], linewidth=2, label="Start Acc (tok)")
axes[0,1].plot(list(epochs_range), [m["end_acc_tok"] for m in val_hist], linewidth=2, label="End Acc (tok)")
axes[0,1].set_title("Token Accuracy")
axes[0,1].set_xlabel("Epoch")
axes[0,1].set_ylabel("Accuracy")
axes[0,1].grid(True, alpha=0.3)
axes[0,1].legend()

# EM/F1
axes[1,0].plot(list(epochs_range), tok_em_curve, linewidth=2, label="Token EM (idx)")
if any(v is not None for v in squad_em_curve):
    axes[1,0].plot(list(epochs_range), [v if v is not None else np.nan for v in squad_em_curve], linewidth=2, label="SQuAD EM (text)")
if any(v is not None for v in squad_f1_curve):
    axes[1,0].plot(list(epochs_range), [v if v is not None else np.nan for v in squad_f1_curve], linewidth=2, label="SQuAD F1 (text)")
axes[1,0].set_title("Exact Match / F1")
axes[1,0].set_xlabel("Epoch")
axes[1,0].grid(True, alpha=0.3)
axes[1,0].legend()

# LR (per epoch)
axes[1,1].set_title("Learning Rate (per epoch)")
if LR_HISTORY:
    spe = config.steps_per_epoch
    lr_epoch = [LR_HISTORY[min((i+1)*spe - 1, len(LR_HISTORY)-1)] for i in range(len(train_losses))]
    axes[1,1].plot(list(epochs_range), lr_epoch, linewidth=2)
axes[1,1].set_xlabel("Epoch")
axes[1,1].set_ylabel("LR")
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig("holographic_gearbox_training.png", dpi=150, bbox_inches="tight")
plt.savefig("holographic_gearbox_training.pdf", bbox_inches="tight")
plt.show()

print("  ✓ Saved: holographic_gearbox_training.png / .pdf")

# Quick attention visualization sample
def visualize_attention_sample(question: str, context: str, max_len: int = 128):
    model.eval()
    enc = tokenizer(question, context, max_length=max_len, truncation="only_second", padding="max_length", return_tensors="pt")
    input_ids = enc["input_ids"].to(DEVICE)
    attn = enc["attention_mask"].to(DEVICE)

    with torch.no_grad():
        out = model(input_ids, attn)

    w = model.get_attention_weights(-1)
    if w is None:
        print("No attention captured.")
        return

    # average heads
    w2 = w[0].mean(dim=0).detach().cpu().numpy()  # [L,L]
    valid = int(attn[0].sum().item())
    valid = min(valid, 40)

    plt.figure(figsize=(7,6))
    plt.imshow(w2[:valid, :valid], aspect="auto")
    plt.title("Attention Weights (Last Layer, Avg Heads)")
    plt.xlabel("Key pos")
    plt.ylabel("Query pos")
    plt.colorbar()
    plt.tight_layout()
    plt.savefig("holographic_attention_patterns.png", dpi=150, bbox_inches="tight")
    plt.show()

    # decoded answer
    start_probs = F.softmax(out["start_logits"][0], dim=-1).detach().cpu().numpy()
    end_probs   = F.softmax(out["end_logits"][0], dim=-1).detach().cpu().numpy()
    s = int(start_probs.argmax())
    e = int(end_probs.argmax())
    toks = tokenizer.convert_ids_to_tokens(input_ids[0].detach().cpu().tolist())
    if e < s:
        e = s
    print("\nQ:", question)
    print("Pred span:", (s,e))
    print("Pred tokens:", tokenizer.convert_tokens_to_string(toks[s:e+1]))

print("\n  Visualizing attention sample...")
visualize_attention_sample(
    "What is machine learning?",
    "Machine learning is a type of artificial intelligence that allows computers to learn from data without being explicitly programmed."
)

# Save final
torch.save({
    "model_state_dict": model.state_dict(),
    "config": config.__dict__,
    "train_losses": train_losses,
    "val_history": val_hist,
    "best_epoch": best_epoch,
    "vocab_cache_path": VOCAB_CACHE_PATH,
    "vocab_entries": len(vocab_data),
    "vocab_samples": len(vocab_dataset),
}, "holographic_gearbox_final.pt")

print("\n" + "█" * 80)
print("█  TRAINING COMPLETE - SAVED: holographic_gearbox_best.pt / holographic_gearbox_final.pt" + " " * 2 + "█")
print("█" * 80)
