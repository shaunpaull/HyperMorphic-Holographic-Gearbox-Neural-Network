#!/usr/bin/env python3
"""
╔══════════════════════════════════════════════════════════════════════════════╗
║  HYPERMORPHIC HOLOGRAPHIC GEARBOX - COMPLETE EDITION (UPGRADED + FIXED)      ║
║  OFFLINE Dictionary+Thesaurus (WordNet+wordfreq) + SQuAD                     ║
╠══════════════════════════════════════════════════════════════════════════════╣
║  FIXES APPLIED (THIS VERSION)                                               ║
║  1) CONTEXT-ONLY LOSS (CRITICAL)                                            ║
║     - start/end loss is computed ONLY over context tokens (sequence_id==1)  ║
║     - CLS token is ALSO allowed (index 0) to support truncation fallback    ║
║                                                                              ║
║  2) REAL SQuAD EM/F1 (STRING-LEVEL)                                         ║
║     - decode spans to text via offset_mapping + context_mask                ║
║     - evaluate.load("squad") if available                                   ║
║                                                                              ║
║  3) HOLO-RAID FIX (DIFFERENTIABLE) + ATTENTION FIX (SOFTMAX)                ║
║     - as per your upgraded architecture                                     ║
║                                                                              ║
║  Author: Shaun Gerrard                                                       ║
║  License: MIT                                                                ║
╚══════════════════════════════════════════════════════════════════════════════╝

GOOGLE COLAB:
1) Paste into a single cell (or split by CELL headers)
2) Run.
"""

# ════════════════════════════════════════════════════════════════════════════════
# CELL 1: INSTALLATIONS
# ════════════════════════════════════════════════════════════════════════════════

print("█" * 80)
print("█  HYPERMORPHIC HOLOGRAPHIC GEARBOX - FIXED (CTX-LOSS + REAL EM/F1)" + " " * 1 + "█")
print("█" * 80)

print("\n" + "=" * 80)
print("  INSTALLING DEPENDENCIES")
print("=" * 80)

import subprocess
import sys

def install(package: str):
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

packages = [
    "torch", "torchvision", "torchaudio",
    "transformers", "datasets", "tokenizers", "sentencepiece",
    "numpy", "scipy", "matplotlib", "tqdm",
    "nltk", "wordfreq",
    "evaluate",
]

for pkg in packages:
    try:
        install(pkg)
    except Exception as e:
        print(f"  Note: {pkg} - {e}")

print("\n✓ Dependencies installed!")

# ════════════════════════════════════════════════════════════════════════════════
# CELL 2: IMPORTS AND CONFIGURATION
# ════════════════════════════════════════════════════════════════════════════════

print("\n" + "=" * 80)
print("  IMPORTING LIBRARIES")
print("=" * 80)

import os
import re
import json
import math
import random
import itertools
import warnings
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Any

import numpy as np
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from datasets import load_dataset
from transformers import AutoTokenizer, get_linear_schedule_with_warmup

# Offline vocab
import nltk
from nltk.corpus import wordnet as wn
from wordfreq import top_n_list

# SQuAD metric (string-level)
try:
    import evaluate
    SQUAD_METRIC = evaluate.load("squad")
except Exception as _e:
    SQUAD_METRIC = None
    print("  ⚠️ evaluate.load('squad') unavailable. Will skip EM/F1 metric.")

warnings.filterwarnings("ignore")

# Reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\n✓ Device: {DEVICE}")
if torch.cuda.is_available():
    print(f"  GPU: {torch.cuda.get_device_name(0)}")
    print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

PRIME_FREQUENCIES = {
    "tiny":    [3, 5, 7],
    "small":   [11, 13, 17, 19, 23],
    "medium":  [29, 31, 37, 41, 43, 47],
    "large":   [53, 59, 61, 67, 71, 73, 79],
    "default": [31, 37, 41, 43, 47],
    "holoraid":[53, 59, 61, 67, 71],
}

print("✓ All imports successful!")

# ════════════════════════════════════════════════════════════════════════════════
# CELL 3: MODEL ARCHITECTURE (HOLO-RAID FIXED + ATTENTION FIXED)
# ════════════════════════════════════════════════════════════════════════════════

print("\n" + "=" * 80)
print("  BUILDING HYPERMORPHIC HOLOGRAPHIC GEARBOX ARCHITECTURE (UPGRADED)")
print("=" * 80)

class HoloRAIDDifferentiable(nn.Module):
    """
    Differentiable HoloRAID:
      - Encode: prime-frequency shards via sin/cos over x
      - Corrupt: shard dropout (simulated failures)
      - Decode: learnable weighted reconstruction + residual
    """
    def __init__(self,
                 dim: int,
                 primes: List[int] = None,
                 k_threshold: int = 3,
                 dropout_p: float = 0.25,
                 strength: float = 0.35):
        super().__init__()
        self.dim = dim
        self.primes = primes or PRIME_FREQUENCIES["holoraid"]
        self.n = len(self.primes)
        self.k = max(1, min(k_threshold, self.n))
        self.dropout_p = dropout_p
        self.strength = strength

        self.shard_mix = nn.ModuleList([nn.Linear(dim, dim, bias=False) for _ in range(self.n)])
        self.recon_mix = nn.Linear(dim, dim, bias=False)
        self.shard_w = nn.Parameter(torch.ones(self.n))
        self.norm = nn.LayerNorm(dim)

    def _make_shard(self, x: torch.Tensor, p: int, mix: nn.Module) -> torch.Tensor:
        omega = (2.0 * math.pi) / float(p)
        s = torch.sin(omega * x)
        c = torch.cos(omega * x)
        return mix(s + c)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        shards = torch.stack([self._make_shard(x, p, self.shard_mix[i]) for i, p in enumerate(self.primes)], dim=0)

        if self.training and self.dropout_p > 0:
            keep = 1.0 - self.dropout_p
            mask = (torch.rand(self.n, device=x.device) < keep).float()
            if mask.sum().item() < self.k:
                idx = torch.randperm(self.n, device=x.device)[:self.k]
                mask = torch.zeros(self.n, device=x.device)
                mask[idx] = 1.0
        else:
            mask = torch.ones(self.n, device=x.device)

        w = F.softmax(self.shard_w, dim=0) * mask
        w = w / w.sum().clamp_min(1e-6)

        recon = (w.view(self.n, 1, 1, 1) * shards).sum(dim=0)
        recon = self.recon_mix(recon)

        return self.norm(x + self.strength * recon)


class SafeGear(nn.Module):
    def __init__(self, dim: int, n_gears: int = 3):
        super().__init__()
        self.n_gears = n_gears
        self.gear_a = nn.Parameter(torch.ones(n_gears) * 0.5)
        self.gear_b = nn.Parameter(torch.ones(n_gears) * 7.0)
        self.mix = nn.Linear(dim, dim)
        self.norm = nn.LayerNorm(dim)

    def gear_transform(self, x: torch.Tensor, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
        a_pos = F.softplus(a) + 0.1
        b_pos = F.softplus(b) + 2.0
        mod_like = torch.sin(2 * math.pi * x / b_pos) * a_pos
        div_like = x / b_pos
        return mod_like + div_like

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = x
        for i in range(self.n_gears):
            out = self.gear_transform(out, self.gear_a[i], self.gear_b[i])
        out = self.mix(out)
        return self.norm(out + x)


class HoloMix(nn.Module):
    def __init__(self, dim: int, hidden_dim: int = None,
                 frequencies: List[int] = None, alpha: float = 0.30, dropout: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim or dim * 4
        self.frequencies = frequencies or PRIME_FREQUENCIES["default"]
        self.n_freq = len(self.frequencies)
        self.alpha = alpha

        self.W1 = nn.Linear(dim, self.hidden_dim)
        self.W2 = nn.Linear(self.hidden_dim, dim)

        self.amp = nn.Parameter(torch.randn(self.n_freq, self.hidden_dim) * 0.08)
        self.phi = nn.Parameter(torch.rand(self.n_freq, self.hidden_dim) * 2 * math.pi)

        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        h = self.W1(x)
        inter = torch.zeros_like(h)
        for i, p in enumerate(self.frequencies):
            inter = inter + self.amp[i] * torch.sin((2 * math.pi * h / p) + self.phi[i])
        h = h + self.alpha * inter
        h = F.gelu(h)
        h = self.dropout(h)
        out = self.W2(h)
        return self.norm(out)


class HolographicAttention(nn.Module):
    """
    FIX: softmax(scores) over keys.
    """
    def __init__(self, dim: int, n_heads: int = 4, dropout: float = 0.1, frequencies: List[int] = None):
        super().__init__()
        assert dim % n_heads == 0, "dim must be divisible by n_heads"
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        self.frequencies = frequencies or PRIME_FREQUENCIES["default"]
        self.n_freq = len(self.frequencies)

        self.Wq = nn.Linear(dim, dim)
        self.Wk = nn.Linear(dim, dim)
        self.Wv = nn.Linear(dim, dim)
        self.Wo = nn.Linear(dim, dim)

        self.alpha_sin = nn.Parameter(torch.randn(n_heads, self.n_freq) * 0.10)
        self.alpha_cos = nn.Parameter(torch.randn(n_heads, self.n_freq) * 0.10)
        self.phi = nn.Parameter(torch.rand(n_heads, self.n_freq) * 2 * math.pi)
        self.temperature = nn.Parameter(torch.ones(1))

        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(dim)
        self.last_attention_weights = None

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        B, L, D = x.shape
        Q = self.Wq(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)
        K = self.Wk(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)
        V = self.Wv(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)

        sim = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        sim = sim / (self.temperature.abs() + 0.1)

        scores = torch.zeros_like(sim)
        for f_idx, p in enumerate(self.frequencies):
            phase = (2 * math.pi * sim / p) + self.phi[:, f_idx].view(1, self.n_heads, 1, 1)
            scores = scores + self.alpha_sin[:, f_idx].view(1, self.n_heads, 1, 1) * torch.sin(phase)
            scores = scores + self.alpha_cos[:, f_idx].view(1, self.n_heads, 1, 1) * torch.cos(phase)

        if mask is not None:
            if mask.dim() == 2:
                mask = mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(mask == 0, -1e9)

        weights = torch.softmax(scores, dim=-1)
        self.last_attention_weights = weights.detach()

        weights = self.dropout(weights)
        out = torch.matmul(weights, V)
        out = out.transpose(1, 2).contiguous().view(B, L, D)
        out = self.Wo(out)
        return self.norm(out)


class HolographicTransformerBlock(nn.Module):
    def __init__(self, dim: int, n_heads: int = 4, dropout: float = 0.1, frequencies: List[int] = None, ffn_mult: int = 4):
        super().__init__()
        self.attn = HolographicAttention(dim, n_heads, dropout, frequencies)
        self.ffn = HoloMix(dim, dim * ffn_mult, frequencies, dropout=dropout)
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        self.drop = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        h = self.attn(self.norm1(x), mask)
        x = x + self.drop(h)
        h = self.ffn(self.norm2(x))
        x = x + self.drop(h)
        return x


class HolographicGearboxModel(nn.Module):
    def __init__(self,
                 vocab_size: int,
                 dim: int = 256,
                 n_layers: int = 4,
                 n_heads: int = 4,
                 max_seq_len: int = 384,
                 dropout: float = 0.2,
                 frequencies: List[int] = None,
                 use_holoraid: bool = True,
                 holoraid_prob: float = 0.10,
                 holoraid_dropout_p: float = 0.25,
                 holoraid_strength: float = 0.35):
        super().__init__()
        self.max_seq_len = max_seq_len
        self.frequencies = frequencies or PRIME_FREQUENCIES["default"]
        self.use_holoraid = use_holoraid
        self.holoraid_prob = holoraid_prob
        self.dim = dim

        self.token_embed = nn.Embedding(vocab_size, dim)
        self.safegear = SafeGear(dim, n_gears=3)

        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, dim))
        self._init_positions()

        self.holoraid = HoloRAIDDifferentiable(
            dim=dim,
            primes=PRIME_FREQUENCIES["holoraid"],
            k_threshold=3,
            dropout_p=holoraid_dropout_p,
            strength=holoraid_strength,
        )

        self.layers = nn.ModuleList([
            HolographicTransformerBlock(dim, n_heads, dropout, self.frequencies)
            for _ in range(n_layers)
        ])
        self.norm = nn.LayerNorm(dim)
        self.drop = nn.Dropout(dropout)

        self.qa_head = nn.Linear(dim, 2)
        self._init_weights()

    def _init_positions(self):
        pos = torch.arange(self.max_seq_len).float()
        pe = torch.zeros(self.max_seq_len, self.dim)

        for i, p in enumerate(self.frequencies):
            if 2*i < self.dim:
                pe[:, 2*i] = torch.sin(2 * math.pi * pos / p)
            if 2*i+1 < self.dim:
                pe[:, 2*i+1] = torch.cos(2 * math.pi * pos / p)

        start = 2 * len(self.frequencies)
        for i in range(start, self.dim, 2):
            div = 10000 ** (i / self.dim)
            pe[:, i] = torch.sin(pos / div)
            if i+1 < self.dim:
                pe[:, i+1] = torch.cos(pos / div)

        self.pos_embed.data = pe.unsqueeze(0)

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Embedding):
                nn.init.normal_(m.weight, std=0.02)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        B, L = input_ids.shape
        x = self.token_embed(input_ids)
        x = self.safegear(x)
        x = x + self.pos_embed[:, :L, :]

        if self.use_holoraid and self.training and (random.random() < self.holoraid_prob):
            x = self.holoraid(x)

        x = self.drop(x)

        mask = attention_mask.unsqueeze(1).unsqueeze(2) if attention_mask is not None else None
        for layer in self.layers:
            x = layer(x, mask)

        x = self.norm(x)
        logits = self.qa_head(x)
        return {"start_logits": logits[:, :, 0], "end_logits": logits[:, :, 1], "hidden_states": x}

    def get_num_parameters(self) -> int:
        return sum(p.numel() for p in self.parameters())

    def get_attention_weights(self, layer_idx: int = -1) -> Optional[torch.Tensor]:
        if layer_idx == -1:
            layer_idx = len(self.layers) - 1
        if 0 <= layer_idx < len(self.layers):
            return self.layers[layer_idx].attn.last_attention_weights
        return None


print("✓ Model architecture defined.")

# ════════════════════════════════════════════════════════════════════════════════
# CELL 4: DATA (OFFLINE VOCAB + SQuAD) WITH CONTEXT MASK
# ════════════════════════════════════════════════════════════════════════════════

print("\n" + "=" * 80)
print("  LOADING DATA SOURCES (OFFLINE VOCAB + SQuAD)")
print("=" * 80)

def ensure_wordnet():
    try:
        wn.synsets("test")
    except LookupError:
        nltk.download("wordnet")
        nltk.download("omw-1.4")

_word_re = re.compile(r"^[a-z][a-z\-']*$")

def wordnet_entry(word: str, max_syn=12, max_ant=8, max_rel=12) -> Optional[Dict[str, Any]]:
    synsets = wn.synsets(word)
    if not synsets:
        return None

    defs: List[str] = []
    for s in synsets[:3]:
        d = s.definition()
        if d and d not in defs:
            defs.append(d)

    synonyms: List[str] = []
    antonyms: List[str] = []
    related: List[str] = []

    for s in synsets:
        for lemma in s.lemmas():
            w = lemma.name().replace("_", " ").lower()
            if w != word and w not in synonyms:
                synonyms.append(w)
            for ant in lemma.antonyms():
                a = ant.name().replace("_", " ").lower()
                if a not in antonyms:
                    antonyms.append(a)

        for h in s.hypernyms()[:3]:
            for lemma in h.lemmas()[:3]:
                rw = lemma.name().replace("_", " ").lower()
                if rw != word and rw not in related:
                    related.append(rw)
        for h in s.hyponyms()[:3]:
            for lemma in h.lemmas()[:3]:
                rw = lemma.name().replace("_", " ").lower()
                if rw != word and rw not in related:
                    related.append(rw)

    entry: Dict[str, Any] = {"word": word}
    if defs: entry["definitions"] = defs
    if synonyms: entry["synonyms"] = synonyms[:max_syn]
    if antonyms: entry["antonyms"] = antonyms[:max_ant]
    if related: entry["related"] = related[:max_rel]
    return entry if len(entry) > 1 else None

def load_vocab_cache_jsonl(path: str, limit: int) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        return []
    out = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                out.append(json.loads(line))
            except:
                continue
            if len(out) >= limit:
                break
    return out

def save_vocab_cache_jsonl(path: str, entries: List[Dict[str, Any]]):
    with open(path, "w", encoding="utf-8") as f:
        for e in entries:
            f.write(json.dumps(e, ensure_ascii=False) + "\n")

def build_massive_vocab_dataset(
    target_size: int = 50000,
    min_word_len: int = 3,
    top_k_wordfreq: int = 200000,
    cache_path: str = "wordnet_vocab_cache.jsonl",
) -> List[Dict[str, Any]]:
    ensure_wordnet()

    cached = load_vocab_cache_jsonl(cache_path, target_size)
    if len(cached) >= target_size:
        print(f"  ✓ Loaded vocab cache: {len(cached)} entries from {cache_path}")
        return cached[:target_size]

    if cached:
        print(f"  ✓ Partial cache found: {len(cached)} entries. Continuing build...")

    candidates = top_n_list("en", top_k_wordfreq)

    dataset = list(cached)
    seen = {d["word"] for d in dataset if "word" in d}

    for w in tqdm(candidates, desc="Building WordNet vocab"):
        w = w.strip().lower()
        if w in seen:
            continue
        if len(w) < min_word_len:
            continue
        if not _word_re.match(w):
            continue

        seen.add(w)
        entry = wordnet_entry(w)
        if entry:
            dataset.append(entry)
        if len(dataset) >= target_size:
            break

    save_vocab_cache_jsonl(cache_path, dataset)
    print(f"  ✓ Saved vocab cache: {len(dataset)} entries → {cache_path}")
    return dataset

def build_context_mask(enc, attention_mask_tensor: torch.Tensor) -> torch.Tensor:
    """
    Returns [L] int mask where:
      - 1 for context tokens (sequence_id == 1)
      - ALSO 1 for CLS token (index 0) so truncation fallback works
      - 0 elsewhere (question, SEP, padding)
    """
    # enc is a fast BatchEncoding for a *single* example
    seq_ids = enc.sequence_ids(0)  # list length L
    m = torch.zeros_like(attention_mask_tensor, dtype=torch.long)
    for i, sid in enumerate(seq_ids):
        if sid == 1:
            m[i] = 1
    m[0] = 1  # allow CLS always
    m = m * attention_mask_tensor.long()  # never allow padding
    return m

def find_answer_token_span(enc, answer_start: int, answer_text: str) -> Tuple[int, int]:
    """
    Robust mapping using offsets + sequence_ids:
      - context only (sequence_id == 1)
      - CLS if not found (often truncation)
    """
    answer_end = answer_start + len(answer_text)
    offsets = enc["offset_mapping"][0].tolist()
    seq_ids = enc.sequence_ids(0)

    cls_index = 0
    start_pos = cls_index
    end_pos = cls_index

    for i, (s, e) in enumerate(offsets):
        if seq_ids[i] != 1:
            continue
        if s == e == 0:
            continue
        if s <= answer_start < e:
            start_pos = i
        if s < answer_end <= e:
            end_pos = i
            break

    if end_pos < start_pos:
        end_pos = start_pos
    return start_pos, end_pos

class VocabularyQADataset(Dataset):
    def __init__(self, vocab_data: List[Dict[str, Any]], tokenizer, max_length: int = 384,
                 max_syn_show: int = 5, max_ant_show: int = 3, max_rel_show: int = 5):
        self.features: List[Dict[str, Any]] = []
        samples = []

        for entry in vocab_data:
            word = entry.get("word", "")
            if not word:
                continue

            defs = entry.get("definitions", [])
            if defs:
                definition = defs[0]
                ctx = f"The word {word} is defined as: {definition}"
                q = f"What is the definition of {word}?"
                samples.append((q, ctx, definition, len(f"The word {word} is defined as: ")))

            syns = entry.get("synonyms", [])
            if syns:
                ans = ", ".join(syns[:max_syn_show])
                ctx = f"Synonyms of {word} include: {ans}. These words have similar meanings."
                q = f"What are synonyms of {word}?"
                samples.append((q, ctx, ans, len(f"Synonyms of {word} include: ")))

            ants = entry.get("antonyms", [])
            if ants:
                ans = ", ".join(ants[:max_ant_show])
                ctx = f"The opposite of {word} is: {ans}. These are antonyms."
                q = f"What is the opposite of {word}?"
                samples.append((q, ctx, ans, len(f"The opposite of {word} is: ")))

            rel = entry.get("related", [])
            if rel:
                ans = ", ".join(rel[:max_rel_show])
                ctx = f"Words related to {word} include: {ans}. They share semantic connections."
                q = f"What words are related to {word}?"
                samples.append((q, ctx, ans, len(f"Words related to {word} include: ")))

        for q, ctx, ans, ans_start in tqdm(samples, desc="Tokenizing Vocab QA"):
            enc = tokenizer(
                q, ctx,
                max_length=max_length,
                truncation="only_second",
                padding="max_length",
                return_offsets_mapping=True,
                return_tensors="pt",
            )
            input_ids = enc["input_ids"].squeeze(0)
            attn_mask = enc["attention_mask"].squeeze(0)
            ctx_mask = build_context_mask(enc, attn_mask)

            sp, ep = find_answer_token_span(enc, ans_start, ans)

            self.features.append({
                "input_ids": input_ids,
                "attention_mask": attn_mask,
                "context_mask": ctx_mask,
                "start_positions": sp,
                "end_positions": ep,
            })

    def __len__(self):
        return len(self.features)

    def __getitem__(self, i):
        f = self.features[i]
        return f

class SQuADTrainDataset(Dataset):
    def __init__(self, tokenizer, split="train", max_length=384, max_samples=None):
        dataset = load_dataset("squad", split=split)
        if max_samples:
            dataset = dataset.select(range(min(max_samples, len(dataset))))

        self.features: List[Dict[str, Any]] = []

        for ex in tqdm(dataset, desc=f"Tokenizing SQuAD ({split})"):
            enc = tokenizer(
                ex["question"], ex["context"],
                max_length=max_length,
                truncation="only_second",
                padding="max_length",
                return_offsets_mapping=True,
                return_tensors="pt",
            )

            input_ids = enc["input_ids"].squeeze(0)
            attn_mask = enc["attention_mask"].squeeze(0)
            ctx_mask = build_context_mask(enc, attn_mask)

            cls = 0
            sp, ep = cls, cls
            if ex["answers"]["answer_start"]:
                astart = ex["answers"]["answer_start"][0]
                atext = ex["answers"]["text"][0]
                sp, ep = find_answer_token_span(enc, astart, atext)

            self.features.append({
                "input_ids": input_ids,
                "attention_mask": attn_mask,
                "context_mask": ctx_mask,
                "start_positions": sp,
                "end_positions": ep,
            })

    def __len__(self):
        return len(self.features)

    def __getitem__(self, i):
        return self.features[i]

class SQuADValDataset(Dataset):
    """
    Validation retains offset_mapping + raw context/id/answers for string EM/F1.
    """
    def __init__(self, tokenizer, split="validation", max_length=384, max_samples=None):
        dataset = load_dataset("squad", split=split)
        if max_samples:
            dataset = dataset.select(range(min(max_samples, len(dataset))))

        self.items: List[Dict[str, Any]] = []

        for ex in tqdm(dataset, desc=f"Tokenizing SQuAD ({split})"):
            enc = tokenizer(
                ex["question"], ex["context"],
                max_length=max_length,
                truncation="only_second",
                padding="max_length",
                return_offsets_mapping=True,
                return_tensors="pt",
            )

            input_ids = enc["input_ids"].squeeze(0)
            attn_mask = enc["attention_mask"].squeeze(0)
            ctx_mask = build_context_mask(enc, attn_mask)

            cls = 0
            sp, ep = cls, cls
            if ex["answers"]["answer_start"]:
                astart = ex["answers"]["answer_start"][0]
                atext = ex["answers"]["text"][0]
                sp, ep = find_answer_token_span(enc, astart, atext)

            self.items.append({
                "input_ids": input_ids,
                "attention_mask": attn_mask,
                "context_mask": ctx_mask,
                "start_positions": sp,
                "end_positions": ep,
                "offset_mapping": enc["offset_mapping"].squeeze(0),  # [L,2]
                "context": ex["context"],
                "id": ex["id"],
                "answers": ex["answers"],
                "question": ex["question"],
            })

    def __len__(self):
        return len(self.items)

    def __getitem__(self, i):
        return self.items[i]

print("\n  Initializing tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", use_fast=True)

VOCAB_CACHE_PATH = "wordnet_vocab_cache.jsonl"
VOCAB_TARGET_ENTRIES = 50000
VOCAB_WORD_FREQ_CANDIDATES = 200000

print("\n  Building offline dictionary + thesaurus (WordNet)…")
vocab_data = build_massive_vocab_dataset(
    target_size=VOCAB_TARGET_ENTRIES,
    top_k_wordfreq=VOCAB_WORD_FREQ_CANDIDATES,
    cache_path=VOCAB_CACHE_PATH
)
print(f"  ✓ Loaded {len(vocab_data)} WordNet vocab entries")

vocab_dataset = VocabularyQADataset(vocab_data, tokenizer, max_length=384)
print(f"  ✓ Created {len(vocab_dataset)} vocab QA samples")

MAX_TRAIN_SAMPLES = 10000
MAX_VAL_SAMPLES   = 2000

squad_train = SQuADTrainDataset(tokenizer, "train", max_length=384, max_samples=MAX_TRAIN_SAMPLES)
squad_val   = SQuADValDataset(tokenizer, "validation", max_length=384, max_samples=MAX_VAL_SAMPLES)

print(f"  ✓ Loaded SQuAD: {len(squad_train)} train, {len(squad_val)} validation")

# ════════════════════════════════════════════════════════════════════════════════
# CELL 5: TRAINING (CONTROLLED MIXING + EARLY STOPPING + REAL EM/F1 + CTX LOSS)
# ════════════════════════════════════════════════════════════════════════════════

print("\n" + "=" * 80)
print("  TRAINING CONFIGURATION")
print("=" * 80)

@dataclass
class TrainingConfig:
    vocab_size: int = 30522
    dim: int = 256
    n_layers: int = 4
    n_heads: int = 4
    max_seq_len: int = 384
    dropout: float = 0.2

    use_holoraid: bool = True
    holoraid_prob: float = 0.10
    holoraid_dropout_p: float = 0.25
    holoraid_strength: float = 0.35

    batch_size: int = 16
    epochs: int = 50
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    warmup_ratio: float = 0.10
    max_grad_norm: float = 1.0

    vocab_batch_ratio: float = 0.30
    steps_per_epoch: int = 800

    eval_max_examples: int = 600  # decode subset for EM/F1 speed

    patience: int = 5
    min_delta: float = 0.001

    save_best: bool = True
    log_every: int = 50

config = TrainingConfig()

if not torch.cuda.is_available():
    config.batch_size = 4
    config.dim = 128
    config.n_layers = 2
    config.epochs = 10
    config.steps_per_epoch = 200
    config.eval_max_examples = 200
    print("  ⚠️ CPU mode - reduced configuration")

print(f"\n  Mix: {int((1-config.vocab_batch_ratio)*100)}% SQuAD / {int(config.vocab_batch_ratio*100)}% Vocab")
print(f"  Steps/epoch cap: {config.steps_per_epoch}")

print("\n  Initializing model...")
model = HolographicGearboxModel(
    vocab_size=config.vocab_size,
    dim=config.dim,
    n_layers=config.n_layers,
    n_heads=config.n_heads,
    max_seq_len=config.max_seq_len,
    dropout=config.dropout,
    frequencies=PRIME_FREQUENCIES["default"],
    use_holoraid=config.use_holoraid,
    holoraid_prob=config.holoraid_prob,
    holoraid_dropout_p=config.holoraid_dropout_p,
    holoraid_strength=config.holoraid_strength,
).to(DEVICE)

print(f"  ✓ Model params: {model.get_num_parameters():,}")

def collate_train(batch):
    return {
        "input_ids": torch.stack([b["input_ids"] for b in batch]),
        "attention_mask": torch.stack([b["attention_mask"] for b in batch]),
        "context_mask": torch.stack([b["context_mask"] for b in batch]),
        "start_positions": torch.tensor([b["start_positions"] for b in batch], dtype=torch.long),
        "end_positions": torch.tensor([b["end_positions"] for b in batch], dtype=torch.long),
    }

def collate_val(batch):
    return {
        "input_ids": torch.stack([b["input_ids"] for b in batch]),
        "attention_mask": torch.stack([b["attention_mask"] for b in batch]),
        "context_mask": torch.stack([b["context_mask"] for b in batch]),
        "start_positions": torch.tensor([b["start_positions"] for b in batch], dtype=torch.long),
        "end_positions": torch.tensor([b["end_positions"] for b in batch], dtype=torch.long),
        "offset_mapping": [b["offset_mapping"] for b in batch],
        "context": [b["context"] for b in batch],
        "id": [b["id"] for b in batch],
        "answers": [b["answers"] for b in batch],
        "question": [b["question"] for b in batch],
    }

squad_loader = DataLoader(squad_train, batch_size=config.batch_size, shuffle=True, collate_fn=collate_train, num_workers=0)
vocab_loader = DataLoader(vocab_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_train, num_workers=0)
val_loader   = DataLoader(squad_val,   batch_size=config.batch_size, shuffle=False, collate_fn=collate_val,   num_workers=0)

no_decay = ["bias", "LayerNorm.weight", "norm.weight"]
params_decay, params_nodecay = [], []
for n, p in model.named_parameters():
    (params_nodecay if any(nd in n for nd in no_decay) else params_decay).append(p)

optimizer = torch.optim.AdamW(
    [{"params": params_decay, "weight_decay": config.weight_decay},
     {"params": params_nodecay, "weight_decay": 0.0}],
    lr=config.learning_rate
)

total_steps = config.steps_per_epoch * config.epochs
warmup_steps = int(total_steps * config.warmup_ratio)
scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)

LR_HISTORY = []

def mixed_batch_stream(squad_loader, vocab_loader, vocab_ratio: float):
    squad_iter = itertools.cycle(squad_loader)
    vocab_iter = itertools.cycle(vocab_loader)
    while True:
        if random.random() < vocab_ratio:
            yield "vocab", next(vocab_iter)
        else:
            yield "squad", next(squad_iter)

def apply_context_mask_to_logits(start_logits: torch.Tensor,
                                 end_logits: torch.Tensor,
                                 context_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    start_logits/end_logits: [B,L]
    context_mask: [B,L] (1 allowed, 0 masked)
    """
    neg = torch.tensor(-1e9, device=start_logits.device, dtype=start_logits.dtype)
    start_m = start_logits.masked_fill(context_mask == 0, neg)
    end_m   = end_logits.masked_fill(context_mask == 0, neg)
    return start_m, end_m

def train_epoch(epoch: int) -> float:
    model.train()
    total_loss = 0.0
    n = 0
    stream = mixed_batch_stream(squad_loader, vocab_loader, config.vocab_batch_ratio)
    pbar = tqdm(range(config.steps_per_epoch), desc=f"Epoch {epoch+1}/{config.epochs}")

    for step in pbar:
        src, batch = next(stream)
        input_ids = batch["input_ids"].to(DEVICE)
        attn      = batch["attention_mask"].to(DEVICE)
        ctx_mask  = batch["context_mask"].to(DEVICE)
        sp        = batch["start_positions"].to(DEVICE)
        ep        = batch["end_positions"].to(DEVICE)

        out = model(input_ids, attn)

        # ★ FIX: context-only masking for loss
        s_logits, e_logits = apply_context_mask_to_logits(out["start_logits"], out["end_logits"], ctx_mask)
        loss = 0.5 * (F.cross_entropy(s_logits, sp) + F.cross_entropy(e_logits, ep))

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

        lr = scheduler.get_last_lr()[0]
        LR_HISTORY.append(lr)

        total_loss += loss.item()
        n += 1

        if (step + 1) % config.log_every == 0:
            pbar.set_postfix({"src": src, "loss": f"{loss.item():.4f}", "avg": f"{total_loss/n:.4f}", "lr": f"{lr:.2e}"})
        else:
            pbar.set_postfix({"src": src, "avg": f"{total_loss/n:.4f}", "lr": f"{lr:.2e}"})

    return total_loss / max(n, 1)

def best_span_from_logits(start_logits: np.ndarray,
                          end_logits: np.ndarray,
                          allowed_indices: np.ndarray,
                          max_answer_len: int = 30,
                          n_best: int = 20) -> Tuple[int, int]:
    """
    Choose best (start,end) restricted to allowed indices (context + CLS).
    """
    # top-n from allowed
    s_candidates = allowed_indices[np.argsort(start_logits[allowed_indices])[::-1][:n_best]]
    e_candidates = allowed_indices[np.argsort(end_logits[allowed_indices])[::-1][:n_best]]

    best_score = -1e18
    best_s, best_e = int(allowed_indices[0]), int(allowed_indices[0])

    for s in s_candidates:
        for e in e_candidates:
            if e < s:
                continue
            if (e - s + 1) > max_answer_len:
                continue
            score = float(start_logits[s] + end_logits[e])
            if score > best_score:
                best_score = score
                best_s, best_e = int(s), int(e)

    return best_s, best_e

def decode_batch_predictions(batch, start_logits_t, end_logits_t) -> Tuple[List[Dict[str, str]], List[Dict[str, Any]]]:
    """
    Decode predicted spans to text using stored offset_mapping + context_mask.
    """
    preds, refs = [], []

    for i in range(len(batch["id"])):
        ctx = batch["context"][i]
        off = batch["offset_mapping"][i].cpu().numpy()  # [L,2]
        cm  = batch["context_mask"][i].cpu().numpy().astype(np.int32)  # [L]
        allowed = np.where(cm == 1)[0]
        if allowed.size == 0:
            pred_text = ""
        else:
            s_logits = start_logits_t[i].detach().cpu().numpy()
            e_logits = end_logits_t[i].detach().cpu().numpy()
            s, e = best_span_from_logits(s_logits, e_logits, allowed)

            # If CLS chosen, treat as empty (mostly only hits when truncated label is CLS)
            if s == 0 and e == 0:
                pred_text = ""
            else:
                s_char, _ = off[s]
                _, e_char = off[e]
                if (s_char == 0 and e_char == 0) or (e_char < s_char):
                    pred_text = ""
                else:
                    pred_text = ctx[s_char:e_char].strip()

        preds.append({"id": batch["id"][i], "prediction_text": pred_text})
        refs.append({"id": batch["id"][i], "answers": batch["answers"][i]})

    return preds, refs

def evaluate_model() -> Dict[str, float]:
    model.eval()
    total_loss = 0.0
    total = 0
    tok_em = 0
    start_acc = 0
    end_acc = 0

    all_preds, all_refs = [], []
    decoded = 0

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluating", leave=False):
            input_ids = batch["input_ids"].to(DEVICE)
            attn      = batch["attention_mask"].to(DEVICE)
            ctx_mask  = batch["context_mask"].to(DEVICE)
            sp        = batch["start_positions"].to(DEVICE)
            ep        = batch["end_positions"].to(DEVICE)

            out = model(input_ids, attn)

            # ★ FIX: context-only masking for loss + predictions
            s_logits, e_logits = apply_context_mask_to_logits(out["start_logits"], out["end_logits"], ctx_mask)
            loss = 0.5 * (F.cross_entropy(s_logits, sp) + F.cross_entropy(e_logits, ep))

            bs = input_ids.size(0)
            total_loss += loss.item() * bs
            total += bs

            s_pred = s_logits.argmax(dim=-1)
            e_pred = e_logits.argmax(dim=-1)

            start_acc += (s_pred == sp).sum().item()
            end_acc   += (e_pred == ep).sum().item()
            tok_em    += ((s_pred == sp) & (e_pred == ep)).sum().item()

            if SQUAD_METRIC is not None and decoded < config.eval_max_examples:
                # decode with stored python-side batch fields
                preds, refs = decode_batch_predictions(batch, s_logits, e_logits)
                all_preds.extend(preds)
                all_refs.extend(refs)
                decoded += bs

    metrics = {
        "loss": total_loss / max(total, 1),
        "start_acc_tok": start_acc / max(total, 1),
        "end_acc_tok": end_acc / max(total, 1),
        "exact_match_tok": tok_em / max(total, 1),
    }

    if SQUAD_METRIC is not None and all_preds:
        squad_scores = SQUAD_METRIC.compute(predictions=all_preds, references=all_refs)
        metrics["squad_em"] = float(squad_scores["exact_match"])
        metrics["squad_f1"] = float(squad_scores["f1"])

    return metrics

print("\n" + "=" * 80)
print("  STARTING TRAINING (EARLY STOPPING)")
print("=" * 80)

train_losses, val_hist = [], []
best_val = float("inf")
best_em = -1.0
best_epoch = 0
pat = 0

for epoch in range(config.epochs):
    tr = train_epoch(epoch)
    train_losses.append(tr)

    vm = evaluate_model()
    val_hist.append(vm)

    print(f"\n  Epoch {epoch+1}/{config.epochs} Summary:")
    print(f"    Train Loss:       {tr:.4f}")
    print(f"    Val Loss:         {vm['loss']:.4f}")
    print(f"    Token Start Acc:  {vm['start_acc_tok']:.4f}")
    print(f"    Token End Acc:    {vm['end_acc_tok']:.4f}")
    print(f"    Token EM (idx):   {vm['exact_match_tok']:.4f}")
    if "squad_em" in vm:
        print(f"    SQuAD EM (text):  {vm['squad_em']:.2f}")
        print(f"    SQuAD F1 (text):  {vm['squad_f1']:.2f}")

    improved = False
    if vm["loss"] < best_val - config.min_delta:
        best_val = vm["loss"]
        improved = True

    cur_em = vm.get("squad_em", vm["exact_match_tok"])
    if cur_em > best_em:
        best_em = cur_em
        improved = True

    if improved:
        best_epoch = epoch + 1
        pat = 0
        if config.save_best:
            torch.save({
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "val_metrics": vm,
                "config": config.__dict__,
                "vocab_cache_path": VOCAB_CACHE_PATH,
                "vocab_entries": len(vocab_data),
                "vocab_samples": len(vocab_dataset),
            }, "holographic_gearbox_best.pt")
            print(f"    ✓ New best model saved! (val_loss={best_val:.4f}, best_em={best_em:.4f})")
    else:
        pat += 1
        print(f"    No improvement ({pat}/{config.patience})")

    if pat >= config.patience:
        print(f"\n  Early stopping at epoch {epoch+1}. Best epoch: {best_epoch}")
        break

print("\n✓ Training complete!")

# ════════════════════════════════════════════════════════════════════════════════
# CELL 6: VISUALIZATION + QUICK ATTENTION VIEW
# ════════════════════════════════════════════════════════════════════════════════

print("\n" + "=" * 80)
print("  VISUALIZATION AND ANALYSIS")
print("=" * 80)

epochs_range = range(1, len(train_losses) + 1)

val_loss_curve = [m["loss"] for m in val_hist]
tok_em_curve = [m["exact_match_tok"] for m in val_hist]
squad_em_curve = [m.get("squad_em", None) for m in val_hist]
squad_f1_curve = [m.get("squad_f1", None) for m in val_hist]

fig, axes = plt.subplots(2, 2, figsize=(14, 9))

axes[0,0].plot(list(epochs_range), train_losses, linewidth=2, label="Train Loss")
axes[0,0].plot(list(epochs_range), val_loss_curve, linewidth=2, label="Val Loss")
axes[0,0].axvline(best_epoch, linestyle="--", label=f"Best ({best_epoch})")
axes[0,0].set_title("Loss")
axes[0,0].set_xlabel("Epoch")
axes[0,0].set_ylabel("Loss")
axes[0,0].grid(True, alpha=0.3)
axes[0,0].legend()

axes[0,1].plot(list(epochs_range), [m["start_acc_tok"] for m in val_hist], linewidth=2, label="Start Acc (tok)")
axes[0,1].plot(list(epochs_range), [m["end_acc_tok"] for m in val_hist], linewidth=2, label="End Acc (tok)")
axes[0,1].set_title("Token Accuracy")
axes[0,1].set_xlabel("Epoch")
axes[0,1].set_ylabel("Accuracy")
axes[0,1].grid(True, alpha=0.3)
axes[0,1].legend()

axes[1,0].plot(list(epochs_range), tok_em_curve, linewidth=2, label="Token EM (idx)")
if any(v is not None for v in squad_em_curve):
    axes[1,0].plot(list(epochs_range), [v if v is not None else np.nan for v in squad_em_curve], linewidth=2, label="SQuAD EM (text)")
if any(v is not None for v in squad_f1_curve):
    axes[1,0].plot(list(epochs_range), [v if v is not None else np.nan for v in squad_f1_curve], linewidth=2, label="SQuAD F1 (text)")
axes[1,0].set_title("Exact Match / F1")
axes[1,0].set_xlabel("Epoch")
axes[1,0].grid(True, alpha=0.3)
axes[1,0].legend()

axes[1,1].set_title("Learning Rate (per epoch)")
if LR_HISTORY:
    spe = config.steps_per_epoch
    lr_epoch = [LR_HISTORY[min((i+1)*spe - 1, len(LR_HISTORY)-1)] for i in range(len(train_losses))]
    axes[1,1].plot(list(epochs_range), lr_epoch, linewidth=2)
axes[1,1].set_xlabel("Epoch")
axes[1,1].set_ylabel("LR")
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig("holographic_gearbox_training.png", dpi=150, bbox_inches="tight")
plt.savefig("holographic_gearbox_training.pdf", bbox_inches="tight")
plt.show()

print("  ✓ Saved: holographic_gearbox_training.png / .pdf")

def visualize_attention_sample(question: str, context: str, max_len: int = 128):
    model.eval()
    enc = tokenizer(question, context, max_length=max_len, truncation="only_second", padding="max_length",
                    return_offsets_mapping=True, return_tensors="pt")
    input_ids = enc["input_ids"].to(DEVICE)
    attn = enc["attention_mask"].to(DEVICE)
    ctx_mask = build_context_mask(enc, enc["attention_mask"].squeeze(0)).unsqueeze(0).to(DEVICE)

    with torch.no_grad():
        out = model(input_ids, attn)

    w = model.get_attention_weights(-1)
    if w is None:
        print("No attention captured.")
        return

    w2 = w[0].mean(dim=0).detach().cpu().numpy()
    valid = int(attn[0].sum().item())
    valid = min(valid, 40)

    plt.figure(figsize=(7,6))
    plt.imshow(w2[:valid, :valid], aspect="auto")
    plt.title("Attention Weights (Last Layer, Avg Heads)")
    plt.xlabel("Key pos")
    plt.ylabel("Query pos")
    plt.colorbar()
    plt.tight_layout()
    plt.savefig("holographic_attention_patterns.png", dpi=150, bbox_inches="tight")
    plt.show()

    # decode using masked logits (context-only)
    s_logits, e_logits = apply_context_mask_to_logits(out["start_logits"], out["end_logits"], ctx_mask)
    s = int(s_logits[0].argmax().item())
    e = int(e_logits[0].argmax().item())
    if e < s:
        e = s
    offsets = enc["offset_mapping"][0].cpu().numpy()
    s_char, _ = offsets[s]
    _, e_char = offsets[e]
    pred = "" if (s == 0 and e == 0) else context[s_char:e_char].strip()

    print("\nQ:", question)
    print("Pred span:", (s,e))
    print("Pred text:", pred)

print("\n  Visualizing attention sample...")
visualize_attention_sample(
    "What is machine learning?",
    "Machine learning is a type of artificial intelligence that allows computers to learn from data without being explicitly programmed."
)

torch.save({
    "model_state_dict": model.state_dict(),
    "config": config.__dict__,
    "train_losses": train_losses,
    "val_history": val_hist,
    "best_epoch": best_epoch,
    "vocab_cache_path": VOCAB_CACHE_PATH,
    "vocab_entries": len(vocab_data),
    "vocab_samples": len(vocab_dataset),
}, "holographic_gearbox_final.pt")

print("\n" + "█" * 80)
print("█  TRAINING COMPLETE - SAVED: holographic_gearbox_best.pt / holographic_gearbox_final.pt" + " " * 2 + "█")
print("█" * 80)
